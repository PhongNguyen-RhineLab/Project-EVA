<div align="center">

# ğŸ­ Project EVA

**Empathic Voice Assistant** - Trá»£ lÃ½ áº£o Tháº¥u cáº£m sá»­ dá»¥ng MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n vÃ  PhÃ¢n tÃ­ch Giá»ng nÃ³i Äa nhÃ£n

![EVA Banner](https://github.com/user-attachments/assets/4a389759-37be-4c2f-a75c-e4b4e510dcc2)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Research-green.svg)]()
[![Status](https://img.shields.io/badge/Status-In%20Development-yellow.svg)]()

</div>

## ğŸ“‹ Tá»•ng quan

Project EVA lÃ  má»™t há»‡ thá»‘ng AI tiÃªn tiáº¿n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»— trá»£ ngÆ°á»i cÃ³ rá»‘i loáº¡n tÃ¢m lÃ½ thÃ´ng qua viá»‡c phÃ¢n tÃ­ch cáº£m xÃºc tá»« giá»ng nÃ³i vÃ  cung cáº¥p pháº£n há»“i tháº¥u cáº£m, phÃ¹ há»£p vá»›i tráº¡ng thÃ¡i tÃ¢m lÃ½ cá»§a ngÆ°á»i dÃ¹ng.

### âœ¨ TÃ­nh nÄƒng chÃ­nh

Há»‡ thá»‘ng EVA káº¿t há»£p ba kháº£ nÄƒng cá»‘t lÃµi:

1. **ğŸ¤ Speech-to-Text (STT)** - Chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n Ä‘á»ƒ hiá»ƒu ná»™i dung cÃ¢u chuyá»‡n
2. **ğŸ˜Š Speech Emotion Recognition (SER)** - PhÃ¢n tÃ­ch cáº£m xÃºc phá»©c há»£p qua Ä‘áº·c tÃ­nh Ã¢m há»c (vÃ­ dá»¥: Vui 30%, Buá»“n 70%)
3. **ğŸ¤– LLM Integration** - Táº¡o pháº£n há»“i tháº¥u cáº£m dá»±a trÃªn cáº£ ná»™i dung vÃ  tráº¡ng thÃ¡i cáº£m xÃºc
4. **ğŸ”Š Text-to-Speech (TTS)** - Pháº£n há»“i báº±ng giá»ng nÃ³i tá»± nhiÃªn vá»›i há»— trá»£ tiáº¿ng Viá»‡t (ElevenLabs, Edge TTS)

---

## ğŸ—ï¸ Kiáº¿n trÃºc Há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Giá»ng nÃ³i ngÆ°á»i    â”‚
â”‚      dÃ¹ng           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                      â”‚
           â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   STT        â”‚      â”‚     SER      â”‚
    â”‚  (Whisper)   â”‚      â”‚  (Beta-VAE)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚
           â”‚  VÄƒn báº£n            â”‚  Vector cáº£m xÃºc
           â”‚                     â”‚  [0.3, 0.7, ...]
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Prompt Engine      â”‚
           â”‚  (Context-Aware)    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   LLM (Gemma)       â”‚
           â”‚   Táº¡o pháº£n há»“i      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   TTS               â”‚
           â”‚   Pháº£n há»“i báº±ng     â”‚
           â”‚   giá»ng nÃ³i         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Module SER - Speech Emotion Recognition

### MÃ´ hÃ¬nh: Multi-task Beta-VAE

#### ğŸ”§ Kiáº¿n trÃºc

**Encoder:**
- CNN (Conv2D â†’ BatchNorm â†’ MaxPooling)
- LSTM/GRU cho phá»¥ thuá»™c thá»i gian
- Táº¡o latent space: `Î¼` (mean) vÃ  `log_var` (variance)

**Latent Space:**
- Chiá»u: 32-128 dimensions
- Sampling: `z = Î¼ + Ïƒ * Îµ` (reparameterization trick)

**Decoder:**
- TÃ¡i táº¡o spectrogram tá»« latent vector `z`

**Classifier Head:**
- Dense layers + Dropout
- Output: Sigmoid activation (multi-label)
- 8 cáº£m xÃºc Ä‘á»“ng thá»i

#### ğŸ“Š 8 Cáº£m xÃºc cÆ¡ báº£n

| Cáº£m xÃºc | Tiáº¿ng Anh | MÃ´ táº£ |
|---------|-----------|-------|
| ğŸ˜ | Neutral | Trung tÃ­nh |
| ğŸ˜Œ | Calm | BÃ¬nh tÄ©nh |
| ğŸ˜Š | Happy | Vui váº» |
| ğŸ˜¢ | Sad | Buá»“n bÃ£ |
| ğŸ˜  | Angry | Tá»©c giáº­n |
| ğŸ˜¨ | Fearful | Sá»£ hÃ£i |
| ğŸ¤¢ | Disgust | GhÃª tá»Ÿm |
| ğŸ˜² | Surprised | Ngáº¡c nhiÃªn |

> âš ï¸ **LÆ°u Ã½:** MÃ´ hÃ¬nh hiá»‡n táº¡i khÃ´ng náº¯m báº¯t tá»‘t cÃ¡c cáº£m xÃºc xÃ£ há»™i phá»©c táº¡p nhÆ° tá»™i lá»—i, xáº¥u há»•, tá»± hÃ o, ghen tá»‹.

#### ğŸ”¬ Loss Function

```python
L_total = Î±Â·L_classification + Î²Â·L_kld + Î³Â·L_reconstruction

Trong Ä‘Ã³:
â€¢ L_classification = BCE(y_true, y_pred)
â€¢ L_reconstruction = MSE(x_true, x_reconstructed)  
â€¢ L_kld = -0.5Â·Î£(1 + log_var - Î¼Â² - exp(log_var))
```

**Hyperparameters:**
- `Î±`: Trá»ng sá»‘ classification (máº·c Ä‘á»‹nh: 1.0)
- `Î²`: Trá»ng sá»‘ KL divergence (máº·c Ä‘á»‹nh: 0.5, cÃ³ warmup)
- `Î³`: Trá»ng sá»‘ reconstruction (máº·c Ä‘á»‹nh: 0.1)

---

## ğŸ”Œ Module LLM - Context-Aware Prompt

### Prompt Template

```
Bá»‘i cáº£nh há»‡ thá»‘ng: 
Báº¡n lÃ  EVA - trá»£ lÃ½ áº£o tháº¥u cáº£m Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»— trá»£ 
ngÆ°á»i cÃ³ rá»‘i loáº¡n tÃ¢m lÃ½.

PhÃ¢n tÃ­ch cáº£m xÃºc tá»« giá»ng nÃ³i:
NgÆ°á»i dÃ¹ng Ä‘ang cáº£m tháº¥y: Buá»“n bÃ£ (70%), Má»‡t má»i (40%)

HÆ°á»›ng dáº«n pháº£n há»“i:
â€¢ Thá»ƒ hiá»‡n sá»± Ä‘á»“ng cáº£m vÃ  tháº¥u hiá»ƒu
â€¢ CÃ´ng nháº­n tráº¡ng thÃ¡i cáº£m xÃºc má»™t cÃ¡ch tá»± nhiÃªn
â€¢ TrÃ¡nh Ä‘Æ°a ra lá»i khuyÃªn trá»±c tiáº¿p trá»« khi Ä‘Æ°á»£c há»i
â€¢ Sá»­ dá»¥ng giá»ng Ä‘iá»‡u áº¥m Ã¡p, thÃ¢n thiá»‡n
â€¢ Giá»¯ pháº£n há»“i ngáº¯n gá»n nhÆ°ng Ã½ nghÄ©a

Ná»™i dung ngÆ°á»i dÃ¹ng: [VÄƒn báº£n tá»« STT]

Pháº£n há»“i tháº¥u cáº£m cá»§a báº¡n:
```

---

## ğŸ—‚ï¸ Cáº¥u trÃºc Dá»± Ã¡n

```
Project-EVA/
â”œâ”€â”€ ğŸ“ VAE/                      # MÃ´ hÃ¬nh Speech Emotion Recognition
â”‚   â”œâ”€â”€ model.py                 # Kiáº¿n trÃºc Beta-VAE
â”‚   â”œâ”€â”€ train.py                 # Script huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ train_kaggle.py          # Training cho Kaggle
â”‚   â”œâ”€â”€ dataset.py               # Dataset loader cÆ¡ báº£n
â”‚   â”œâ”€â”€ dataset_augmented.py     # Dataset vá»›i augmentation
â”‚   â”œâ”€â”€ inference.py             # Inference & LLM integration
â”‚   â”œâ”€â”€ evaluate_model.py        # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
â”‚   â””â”€â”€ devlog.txt               # Lá»‹ch sá»­ phÃ¡t triá»ƒn
â”‚
â”œâ”€â”€ ğŸ“ STT/                      # Speech-to-Text Module
â”‚   â””â”€â”€ stt_engine.py            # STT engine (Whisper/Vosk)
â”‚
â”œâ”€â”€ ğŸ“ LLM/                      # Large Language Model Module
â”‚   â””â”€â”€ llm_engine.py            # LLM engine (Groq, Gemini, etc.)
â”‚
â”œâ”€â”€ ğŸ“ TTS/                      # Text-to-Speech Module
â”‚   â”œâ”€â”€ __init__.py              # Module exports
â”‚   â””â”€â”€ tts_engine.py            # TTS engine (ElevenLabs, Edge, gTTS)
â”‚
â”œâ”€â”€ ğŸ“ Pipeline/                 # EVA Pipeline Orchestrator
â”‚   â””â”€â”€ eva_pipeline.py          # Main pipeline (STT + SER + LLM + TTS)
â”‚
â”œâ”€â”€ ğŸ“ API/                      # REST API Server
â”‚   â””â”€â”€ eva_api.py               # FastAPI backend
â”‚
â”œâ”€â”€ ğŸ“ Dataset/                  # Scripts quáº£n lÃ½ dataset
â”‚   â”œâ”€â”€ download_datasets.py     # Táº£i RAVDESS, TESS, CREMA-D
â”‚   â”œâ”€â”€ prepare_dataset.py       # Chuáº©n bá»‹ & chia dataset
â”‚   â”œâ”€â”€ extract_dataset.py       # Giáº£i nÃ©n manual
â”‚   â”œâ”€â”€ extract_dataset_colab.py # Cho Google Colab
â”‚   â””â”€â”€ kaggle_organize_datasets.py  # Cho Kaggle
â”‚
â”œâ”€â”€ ğŸ“ prompts/                  # Prompt templates
â”‚   â”œâ”€â”€ system_context.txt       # System context for LLM
â”‚   â””â”€â”€ response_guidelines.txt  # Emotion-specific guidelines
â”‚
â”œâ”€â”€ ğŸ“ EVA_Dataset/              # Dataset Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ processed_audio/         # Audio files (.wav)
â”‚   â””â”€â”€ labels/                  # Label files (.csv)
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/              # Model checkpoints
â”œâ”€â”€ ğŸ“ logs/                     # Training logs
â”œâ”€â”€ ğŸ“ plots/                    # Visualization plots
â”œâ”€â”€ ğŸ“ evaluation_results/       # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
â”‚
â”œâ”€â”€ ğŸ“„ .env.example              # Environment variables template
â”œâ”€â”€ ğŸ“„ requirements.txt          # Dependencies
â””â”€â”€ ğŸ“„ README.md                 # Documentation
```

---

## ğŸš€ HÆ°á»›ng dáº«n Sá»­ dá»¥ng

### 1ï¸âƒ£ CÃ i Ä‘áº·t Dependencies

```bash
pip install -r requirements.txt
```

**ThÆ° viá»‡n chÃ­nh:**
- `torch>=2.0.0` - Deep learning framework
- `librosa>=0.10.0` - Audio processing
- `openai-whisper` - Speech-to-text
- `groq`, `google-generativeai` - LLM APIs
- `elevenlabs` - Text-to-speech (premium)
- `edge-tts` - Text-to-speech (free)
- `fastapi`, `uvicorn` - REST API
- `numpy`, `pandas`, `scikit-learn` - Data processing
- `tqdm`, `matplotlib`, `seaborn` - Utilities & visualization

### 1.5ï¸âƒ£ Cáº¥u hÃ¬nh API Keys

```bash
# Copy template vÃ  Ä‘iá»n API keys
cp .env.example .env

# Edit .env vá»›i API keys cá»§a báº¡n
# - GROQ_API_KEY hoáº·c GEMINI_API_KEY (cho LLM)
# - ELEVENLABS_API_KEY (cho TTS - tÃ¹y chá»n)
```

### 2ï¸âƒ£ Chuáº©n bá»‹ Dataset

#### Option A: Tá»± Ä‘á»™ng download (khuyáº¿n nghá»‹)

```bash
python Dataset/download_datasets.py
```

Dataset sáº½ táº£i vá»:
- **RAVDESS** (~1.5GB) - 1,440 samples
- **TESS** (~400MB) - 2,800 samples  
- **CREMA-D** (~2GB) - 7,442 samples

#### Option B: Manual download

1. Download cÃ¡c dataset tá»« nguá»“n chÃ­nh thá»©c
2. Äáº·t file zip vÃ o thÆ° má»¥c `Dataset/`
3. Cháº¡y:
```bash
python Dataset/extract_dataset.py
```

#### Xá»­ lÃ½ vÃ  chia dataset

```bash
python Dataset/prepare_dataset.py
```

Káº¿t quáº£:
- âœ… Train set (70%): ~8,177 samples
- âœ… Val set (15%): ~1,752 samples
- âœ… Test set (15%): ~1,753 samples

### 3ï¸âƒ£ Huáº¥n luyá»‡n MÃ´ hÃ¬nh

```bash
python VAE/train.py
```

**Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh:**
- Batch size: 32
- Learning rate: 1e-3
- Epochs: 100 (vá»›i early stopping)
- Latent dim: 64
- Augmentation: ON

**Theo dÃµi training:**
```bash
# Xem plots
ls plots/

# Xem logs
cat logs/training_history.json
```

### 4ï¸âƒ£ ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh

```bash
python VAE/evaluate_model.py
```

Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u trong `evaluation_results/`:
- ğŸ“Š Per-class metrics
- ğŸ“ˆ Confusion matrices
- ğŸ” Latent space visualization
- ğŸ“„ Classification report

### 5ï¸âƒ£ Inference - Sá»­ dá»¥ng MÃ´ hÃ¬nh

```python
from VAE.inference import EmotionRecognizer

# Khá»Ÿi táº¡o
recognizer = EmotionRecognizer(
    checkpoint_path="checkpoints/best_model.pth",
    device='cuda'
)

# PhÃ¢n tÃ­ch audio
emotions, dominant, latent = recognizer.predict("audio_file.wav")

# Táº¡o LLM prompt
user_text = "TÃ´i cáº£m tháº¥y ráº¥t má»‡t má»i..."
prompt = recognizer.generate_llm_prompt(user_text, emotions)
```

### 6ï¸âƒ£ Sá»­ dá»¥ng Full Pipeline (STT + SER + LLM + TTS)

```bash
# Xá»­ lÃ½ audio file vá»›i output audio
python Pipeline/eva_pipeline.py audio.wav --output-audio response.mp3

# Sá»­ dá»¥ng ElevenLabs TTS
python Pipeline/eva_pipeline.py audio.wav --tts-backend elevenlabs

# Sá»­ dá»¥ng Edge TTS (miá»…n phÃ­)
python Pipeline/eva_pipeline.py audio.wav --tts-backend edge
```

### 7ï¸âƒ£ Cháº¡y API Server

```bash
# Khá»Ÿi Ä‘á»™ng server
python API/eva_api.py

# Hoáº·c vá»›i uvicorn
uvicorn API.eva_api:app --host 0.0.0.0 --port 8000 --reload
```

**API Endpoints:**
- `POST /process` - Full pipeline (STT + SER + LLM)
- `POST /process/with-audio` - Full pipeline vá»›i TTS audio response
- `POST /synthesize` - Text-to-speech (standalone)
- `GET /tts/voices` - Danh sÃ¡ch voices
- `GET /docs` - Interactive API documentation

---

## ğŸ”Š Module TTS - Text-to-Speech

### Backends há»— trá»£

| Backend | Cháº¥t lÆ°á»£ng | Vietnamese | API Key | Ghi chÃº |
|---------|-----------|------------|---------|---------|
| **ElevenLabs** | Tá»‘t nháº¥t | âœ… | Required | Multilingual v2 |
| **Edge TTS** | Tá»‘t | âœ… | Free | Microsoft voices |
| **gTTS** | CÆ¡ báº£n | âœ… | Free | Google Translate |

### Cáº¥u hÃ¬nh ElevenLabs (Khuyáº¿n nghá»‹)

```bash
# Láº¥y API key táº¡i: https://elevenlabs.io/
export ELEVENLABS_API_KEY=your_key_here

# Hoáº·c thÃªm vÃ o .env
TTS_BACKEND=elevenlabs
ELEVENLABS_API_KEY=your_key_here
```

### Sá»­ dá»¥ng TTS Engine

```python
from TTS.tts_engine import TTSEngine

# Khá»Ÿi táº¡o (auto-detect backend)
tts = TTSEngine(language="vi")

# Synthesize text
response = tts.synthesize("Xin chÃ o! TÃ´i lÃ  EVA.")

# LÆ°u audio
with open("output.mp3", "wb") as f:
    f.write(response.audio_data)

# Hoáº·c lÆ°u trá»±c tiáº¿p
tts.synthesize_to_file("Xin chÃ o!", "output.mp3")
```

### Vietnamese Voices

**ElevenLabs:**
- Sá»­ dá»¥ng model `eleven_multilingual_v2` (tá»± Ä‘á»™ng)
- Há»— trá»£ nhiá»u giá»ng: Adam, Rachel, vÃ  nhiá»u hÆ¡n

**Edge TTS:**
- `vi-VN-HoaiMyNeural` (Female)
- `vi-VN-NamMinhNeural` (Male)

---

## ğŸ“Š Tiá»n xá»­ lÃ½ Audio

### Mel Spectrogram Configuration

```python
sr = 16000              # Sample rate
n_mels = 128            # Mel frequency bins
hop_length = 512        # Frame shift
n_fft = 2048           # FFT window size
duration = 3            # Audio length (seconds)
```

### Data Augmentation (Training only)

- â±ï¸ Time shifting
- ğŸ”Š Noise injection
- ğŸµ Speed perturbation
- ğŸ“‰ Time masking (SpecAugment)
- ğŸ“Š Frequency masking (SpecAugment)

---

## ğŸ“ Training trÃªn Cloud Platforms

### Google Colab

```python
# Upload file colab_extract_datasets.py
!python Dataset/extract_dataset_colab.py

# Prepare dataset
!python Dataset/prepare_dataset.py

# Train
!python VAE/train.py
```

### Kaggle

```python
# Add datasets via UI (RAVDESS, TESS, CREMA-D)
!python Dataset/kaggle_organize_datasets.py

# Prepare & train
!python Dataset/prepare_dataset.py
!python VAE/train_kaggle.py
```

---

## ğŸ”¬ Roadmap

### âœ… Giai Ä‘oáº¡n 1: SER Model
- [x] Thiáº¿t káº¿ kiáº¿n trÃºc Beta-VAE
- [x] Dataset pipeline (RAVDESS, TESS, CREMA-D)
- [x] Training vá»›i augmentation
- [x] Evaluation metrics

### âœ… Giai Ä‘oáº¡n 2: End-to-End Prototype (HoÃ n thÃ nh)
- [x] TÃ­ch há»£p STT (Whisper/Vosk)
- [x] LLM integration (Groq, Gemini, OpenRouter, Ollama)
- [x] Context-aware prompt engine
- [x] TTS module (ElevenLabs, Edge TTS, gTTS)
- [x] REST API (FastAPI)

### ğŸš§ Giai Ä‘oáº¡n 3: Production Ready
- [ ] Web/Mobile UI
- [ ] Dataset tiáº¿ng Viá»‡t
- [ ] Fine-tuning cho use case cá»¥ thá»ƒ
- [ ] A/B testing & user feedback

---

## âš ï¸ ThÃ¡ch thá»©c & Háº¡n cháº¿

| ThÃ¡ch thá»©c | MÃ´ táº£ | Giáº£i phÃ¡p Ä‘á» xuáº¥t |
|-----------|-------|-------------------|
| ğŸ—‚ï¸ **Dá»¯ liá»‡u** | Thiáº¿u dataset tiáº¿ng Viá»‡t cÃ³ gÃ¡n nhÃ£n cáº£m xÃºc | Thu tháº­p & gÃ¡n nhÃ£n dá»¯ liá»‡u ná»™i bá»™ |
| âš–ï¸ **CÃ¢n báº±ng Loss** | Äiá»u chá»‰nh Î±, Î², Î³ phá»©c táº¡p | Grid search + beta warmup |
| â±ï¸ **Äá»™ trá»…** | Pipeline nhiá»u bÆ°á»›c gÃ¢y delay | Tá»‘i Æ°u hÃ³a inference, model compression |
| ğŸ“ **ÄÃ¡nh giÃ¡** | Tháº¥u cáº£m khÃ³ Ä‘o lÆ°á»ng Ä‘á»‹nh lÆ°á»£ng | User studies + qualitative metrics |
| ğŸ­ **Cáº£m xÃºc phá»©c táº¡p** | KhÃ´ng náº¯m báº¯t Ä‘Æ°á»£c cáº£m xÃºc xÃ£ há»™i báº­c cao | Má»Ÿ rá»™ng taxonomy, larger models |

---

## ğŸ› ï¸ CÃ´ng nghá»‡ Sá»­ dá»¥ng

### Core Technologies

| Component | Technology | Version |
|-----------|-----------|---------|
| ğŸ§  **Deep Learning** | PyTorch | 2.0+ |
| ğŸµ **Audio Processing** | Librosa | 0.10+ |
| ğŸ—£ï¸ **STT** | Whisper / Vosk | Latest |
| ğŸ¤– **LLM** | Groq, Gemini, OpenRouter, Ollama | Latest |
| ğŸ”Š **TTS** | ElevenLabs, Edge TTS, gTTS | Latest |
| ğŸ“Š **Data Science** | NumPy, Pandas, Scikit-learn | Latest |
| ğŸ“ˆ **Visualization** | Matplotlib, Seaborn | Latest |

### Deployment Stack (Planned)

- ğŸ³ **Container**: Docker
- ğŸŒ **API**: FastAPI + Uvicorn
- ğŸ“± **Mobile**: React Native
- â˜ï¸ **Cloud**: AWS/GCP/Azure

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

### Datasets
- [RAVDESS](https://zenodo.org/record/1188976) - Ryerson Audio-Visual Database
- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487) - Toronto Emotional Speech Set
- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D) - Crowd Sourced Emotional Multimodal Actors

### Papers
- Higgins et al. (2017) - "Î²-VAE: Learning Basic Visual Concepts"
- Mirsamadi et al. (2017) - "Automatic Speech Emotion Recognition"

---

## ğŸ‘¥ ÄÃ³ng gÃ³p

Má»i Ä‘Ã³ng gÃ³p Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh! Vui lÃ²ng:

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Táº¡o Pull Request

---

## ğŸ“„ License

Project nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch nghiÃªn cá»©u vÃ  giÃ¡o dá»¥c.

---

## ğŸ“§ LiÃªn há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c gÃ³p Ã½, vui lÃ²ng táº¡o issue trÃªn GitHub repository.

---

<div align="center">

**Made with â¤ï¸ for mental health support**

â­ Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y cho má»™t star!

</div>
